import streamlit as st
import streamlit.components.v1 as components
from speech_recognition_service import create_speech_recognition_service
import tempfile
import os

def voice_input_component():
    """è¯­éŸ³è¾“å…¥ç»„ä»¶"""
    
    # HTMLå’ŒJavaScriptä»£ç 
    html_code = """
    <div id="voice-input-container">
        <button id="start-recording" onclick="startRecording()" style="
            background-color: #007bff; 
            color: white; 
            border: none; 
            padding: 10px 20px; 
            border-radius: 5px; 
            cursor: pointer;
            margin: 5px;
        ">ğŸ¤ å¼€å§‹å½•éŸ³</button>
        
        <button id="stop-recording" onclick="stopRecording()" style="
            background-color: #dc3545; 
            color: white; 
            border: none; 
            padding: 10px 20px; 
            border-radius: 5px; 
            cursor: pointer;
            margin: 5px;
            display: none;
        ">â¹ï¸ åœæ­¢å½•éŸ³</button>
        
        <div id="recording-status" style="
            text-align: center; 
            color: red; 
            font-size: 1.2rem; 
            margin: 10px 0;
            display: none;
        ">âºï¸ å½•éŸ³ä¸­...</div>
        
        <div id="transcription-result" style="
            margin: 10px 0; 
            padding: 10px; 
            background-color: #f8f9fa; 
            border-radius: 5px;
            display: none;
        "></div>
    </div>

    <script>
    let mediaRecorder;
    let audioChunks = [];
    let isRecording = false;
    let recordingStartTime;

    async function startRecording() {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            mediaRecorder = new MediaRecorder(stream);
            audioChunks = [];
            
            mediaRecorder.ondataavailable = (event) => {
                audioChunks.push(event.data);
            };
            
            mediaRecorder.onstop = () => {
                const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                const audioUrl = URL.createObjectURL(audioBlob);
                
                // æ˜¾ç¤ºå½•éŸ³ç»“æœ
                document.getElementById('transcription-result').innerHTML = 
                    '<strong>å½•éŸ³å®Œæˆï¼</strong><br>' +
                    '<audio controls><source src="' + audioUrl + '" type="audio/wav"></audio><br>' +
                    '<p>ç”±äºæµè§ˆå™¨é™åˆ¶ï¼Œæ— æ³•ç›´æ¥è¿›è¡Œè¯­éŸ³è¯†åˆ«ã€‚<br>' +
                    'è¯·ä¸‹è½½éŸ³é¢‘æ–‡ä»¶åä¸Šä¼ åˆ°ç³»ç»Ÿè¿›è¡Œè½¬æ¢ã€‚</p>';
                document.getElementById('transcription-result').style.display = 'block';
                
                // åˆ›å»ºä¸‹è½½é“¾æ¥
                const downloadLink = document.createElement('a');
                downloadLink.href = audioUrl;
                downloadLink.download = 'recording.wav';
                downloadLink.textContent = 'ğŸ“¥ ä¸‹è½½å½•éŸ³æ–‡ä»¶';
                downloadLink.style.cssText = 'display: inline-block; margin: 10px; padding: 8px 16px; background-color: #28a745; color: white; text-decoration: none; border-radius: 4px;';
                document.getElementById('transcription-result').appendChild(downloadLink);
            };
            
            mediaRecorder.start();
            isRecording = true;
            recordingStartTime = Date.now();
            
            // æ›´æ–°UI
            document.getElementById('start-recording').style.display = 'none';
            document.getElementById('stop-recording').style.display = 'inline-block';
            document.getElementById('recording-status').style.display = 'block';
            
            // æ›´æ–°å½•éŸ³æ—¶é—´
            updateRecordingTime();
            
        } catch (error) {
            alert('æ— æ³•è®¿é—®éº¦å…‹é£: ' + error.message);
        }
    }
    
    function stopRecording() {
        if (mediaRecorder && isRecording) {
            mediaRecorder.stop();
            isRecording = false;
            
            // æ›´æ–°UI
            document.getElementById('start-recording').style.display = 'inline-block';
            document.getElementById('stop-recording').style.display = 'none';
            document.getElementById('recording-status').style.display = 'none';
        }
    }
    
    function updateRecordingTime() {
        if (isRecording) {
            const elapsed = Math.floor((Date.now() - recordingStartTime) / 1000);
            document.getElementById('recording-status').textContent = `âºï¸ å½•éŸ³ä¸­... ${elapsed}ç§’`;
            setTimeout(updateRecordingTime, 1000);
        }
    }
    </script>
    """
    
    # æ¸²æŸ“ç»„ä»¶
    components.html(html_code, height=300)

def voice_input_section():
    """è¯­éŸ³è¾“å…¥åŒºåŸŸ"""
    st.markdown('<h3 class="section-header">ğŸ¤ è¯­éŸ³è¾“å…¥</h3>', unsafe_allow_html=True)
    
    # è¯­éŸ³è¾“å…¥è¯´æ˜
    st.info("""
    **è¯­éŸ³è¾“å…¥åŠŸèƒ½è¯´æ˜ï¼š**
    1. ç‚¹å‡»"å¼€å§‹å½•éŸ³"æŒ‰é’®å¼€å§‹å½•éŸ³
    2. è¯´è¯å®Œæˆåç‚¹å‡»"åœæ­¢å½•éŸ³"
    3. ä¸‹è½½å½•éŸ³æ–‡ä»¶åä¸Šä¼ åˆ°ç³»ç»Ÿè¿›è¡Œæ–‡å­—è½¬æ¢
    4. æˆ–è€…ç›´æ¥ä½¿ç”¨æ–‡æœ¬è¾“å…¥åŠŸèƒ½
    """)
    
    # æ¸²æŸ“è¯­éŸ³è¾“å…¥ç»„ä»¶
    voice_input_component()
    
    st.markdown("---")
    st.markdown("**æˆ–è€…ç›´æ¥ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼š**")
    
    # æ–‡ä»¶ä¸Šä¼ å™¨
    uploaded_audio = st.file_uploader(
        "ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒWAVã€MP3æ ¼å¼ï¼‰",
        type=['wav', 'mp3'],
        key="audio_uploader"
    )
    
    if uploaded_audio is not None:
        st.success(f"å·²ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶: {uploaded_audio.name}")
        
        # æ˜¾ç¤ºéŸ³é¢‘æ’­æ”¾å™¨
        st.audio(uploaded_audio, format='audio/wav')
        
        # è¯­éŸ³è¯†åˆ«æœåŠ¡é…ç½®
        st.markdown("**è¯­éŸ³è¯†åˆ«æœåŠ¡é…ç½®ï¼š**")
        col1, col2 = st.columns(2)
        
        with col1:
            service_type = st.selectbox(
                "é€‰æ‹©è¯­éŸ³è¯†åˆ«æœåŠ¡ï¼š",
                ["google", "azure", "baidu", "tencent"],
                format_func=lambda x: {
                    "google": "Google Speech Recognition",
                    "azure": "Azure Speech Services", 
                    "baidu": "ç™¾åº¦è¯­éŸ³è¯†åˆ«",
                    "tencent": "è…¾è®¯è¯­éŸ³è¯†åˆ«"
                }[x]
            )
        
        with col2:
            language = st.selectbox(
                "é€‰æ‹©è¯­è¨€ï¼š",
                ["zh-CN", "zh-TW", "en-US", "en-GB"],
                format_func=lambda x: {
                    "zh-CN": "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰",
                    "zh-TW": "ä¸­æ–‡ï¼ˆç¹ä½“ï¼‰", 
                    "en-US": "è‹±è¯­ï¼ˆç¾å›½ï¼‰",
                    "en-GB": "è‹±è¯­ï¼ˆè‹±å›½ï¼‰"
                }[x]
            )
        
        # è½¬æ¢æŒ‰é’®
        if st.button("ğŸµ è½¬æ¢è¯­éŸ³ä¸ºæ–‡å­—", key="convert_audio"):
            with st.spinner("æ­£åœ¨è½¬æ¢è¯­éŸ³..."):
                try:
                    # åˆ›å»ºè¯­éŸ³è¯†åˆ«æœåŠ¡
                    speech_service = create_speech_recognition_service(service_type=service_type)
                    
                    # è½¬æ¢éŸ³é¢‘æ–‡ä»¶
                    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
                        temp_file.write(uploaded_audio.getvalue())
                        temp_audio_path = temp_file.name
                    
                    # è¿›è¡Œè¯­éŸ³è¯†åˆ«
                    transcribed_text = speech_service.transcribe_audio_file(temp_audio_path, language)
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    os.unlink(temp_audio_path)
                    
                    # ä¿å­˜ç»“æœ
                    st.session_state.transcribed_text = transcribed_text
                    st.success("è¯­éŸ³è½¬æ¢å®Œæˆï¼")
                    
                except Exception as e:
                    st.error(f"è¯­éŸ³è½¬æ¢å¤±è´¥: {str(e)}")
                    # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œæä¾›ç¤ºä¾‹æ–‡å­—
                    st.session_state.transcribed_text = "æ‚£è€…ä¸»è¯‰å…³èŠ‚ç–¼ç—›3ä¸ªæœˆï¼Œæ™¨åƒµæ˜æ˜¾ï¼Œä¼´æœ‰çš®ç–¹å’Œå‘çƒ­ç—‡çŠ¶ã€‚"
                    st.info("å·²æä¾›ç¤ºä¾‹æ–‡å­—ï¼Œæ‚¨å¯ä»¥ç¼–è¾‘åä½¿ç”¨ã€‚") 